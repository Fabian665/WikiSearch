{"cells":[{"cell_type":"markdown","id":"a00e032c","metadata":{"id":"hWgiQS0zkWJ5"},"source":["***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"]},{"cell_type":"code","execution_count":1,"id":"5ac36d3a","metadata":{"id":"c0ccf76b","nbgrader":{"grade":false,"grade_id":"cell-Worker_Count","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"cf88b954-f39a-412a-d87e-660833e735b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","cluster-38d1  GCE       4                                             RUNNING  us-central1-a\r\n"]}],"source":["# if the following command generates an error, you probably didn't enable \n","# the cluster security option \"Allow API access to all Google Cloud services\"\n","# under Manage Security → Project Access when setting up the cluster\n","!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"markdown","id":"51cf86c5","metadata":{"id":"01ec9fd3"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":2,"id":"bf199e6a","metadata":{"id":"32b3ec57","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"fc0e315d-21e9-411d-d69c-5b97e4e5d629"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":3,"id":"d8f56ecd","metadata":{"id":"5609143b","nbgrader":{"grade":false,"grade_id":"cell-Imports","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"a24aa24b-aa75-4823-83ca-1d7deef0f0de"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":4,"id":"38a897f2","metadata":{"id":"b10cc999","nbgrader":{"grade":false,"grade_id":"cell-jar","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"8f93a7ec-71e0-49c1-fc81-9af385849a90"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Mar  7 07:40 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":5,"id":"47900073","metadata":{"id":"d3f86f11","nbgrader":{"grade":false,"grade_id":"cell-pyspark-import","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":6,"id":"72bed56b","metadata":{"id":"5be6dc2a","nbgrader":{"grade":false,"grade_id":"cell-spark-version","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"07b4e22b-a252-42fb-fe46-d9050e4e7ca8","scrolled":true},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-38d1-m.c.inforetrievalexercise3.internal:41529\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f102d8d7910>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":7,"id":"cf74826f-66e0-4b93-8a1c-2ad8345d1230","metadata":{"id":"7adc1bf5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'bucket324955145' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()"]},{"cell_type":"code","execution_count":8,"id":"121fe102","metadata":{"id":"04371c88","outputId":"327fe81b-80f4-4b3a-8894-e74720d92e35","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":9,"id":"57c101a8","metadata":{"id":"2d3285d8","scrolled":true},"outputs":[],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":10,"id":"c259c402","metadata":{"id":"2477a5b9"},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex"]},{"cell_type":"code","execution_count":11,"id":"4d97b412-3f77-474d-8a0a-502f5f2fd8c9","metadata":{"id":"a4b6ee29","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)"]},{"cell_type":"code","execution_count":12,"id":"fa14121e","metadata":{},"outputs":[],"source":["# path = '/home/dataproc'\n","for blob in client.list_blobs(bucket_name):\n","    if blob.name.endswith('.pkl'):\n","        file_name = blob.name[blob.name.rfind('/')+1:]\n","        blob.download_to_filename(file_name)"]},{"cell_type":"code","execution_count":13,"id":"53c73591","metadata":{},"outputs":[],"source":["with open('postings_gcp_title_inverted_title_v1.pkl', 'rb') as file:\n","    inverted_title = pickle.load(file)\n","    \n","with open('postings_gcp_text_inverted_text_v1.pkl', 'rb') as file:\n","    inverted_text = pickle.load(file)"]},{"cell_type":"code","execution_count":14,"id":"93b67f86-7875-42b5-8e35-f00c82319ab1","metadata":{},"outputs":[],"source":["from pyspark.sql.types import MapType, StringType, IntegerType \n","from pyspark.sql.functions import create_map, lit"]},{"cell_type":"code","execution_count":15,"id":"ccc97b28","metadata":{},"outputs":[],"source":["def calculate_document_length(postings):\n","    return postings.map(lambda x: (x[1][0], x[1][1])).reduceByKey(lambda a, b: a + b)"]},{"cell_type":"code","execution_count":16,"id":"66a79072","metadata":{},"outputs":[],"source":["# word_counts_text = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","# word_counts_title = doc_title_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","\n","# # calculate document lengths:\n","# title_doc_length = calculate_document_length(word_counts_title)\n","# text_doc_length = calculate_document_length(word_counts_text)\n","\n","# # title_length = title_doc_length.collectAsMap()\n","# # document_length = text_doc_length.collectAsMap()\n","\n","# total_title_length = title_doc_length.map(lambda x: x[1]).sum()\n","# total_title_docs = title_doc_length.count()\n","\n","# # Calculate total length and total number of documents for texts:\n","# total_text_length = text_doc_length.map(lambda x: x[1]).sum()\n","# total_text_docs = text_doc_length.count()\n","\n","# # Calculate avdl:\n","# average_title_length = total_title_length / total_title_docs\n","# average_text_length = total_text_length / total_text_docs"]},{"cell_type":"code","execution_count":null,"id":"23442283-18e0-474b-a22e-cbd9deba36c2","metadata":{},"outputs":[],"source":["text_doc_len = inverted_text.doc_len.copy()\n","title_doc_len = inverted_title.doc_len.copy()\n","\n","# an attempt at making the these into dataframes\n","# mapping_expr = create_map(\"my_map_column\", [lit(k) for k, v in text_doc_len.items()])\n","# df_text_doc_len = df.withColumn(\"value\", mapping_expr[col(\"key\")])"]},{"cell_type":"code","execution_count":null,"id":"483dacbc-2247-42a8-9f41-83faf36610d3","metadata":{},"outputs":[],"source":["# text_idf = sc.broadcast(inverted_text.idf)\n","# title_idf = sc.broadcast(inverted_title.idf)\n","\n","text_idf = inverted_text.idf\n","title_idf = inverted_title.idf"]},{"cell_type":"code","execution_count":null,"id":"8dc18aa3-3c0f-4fd2-952c-1eb1ca3906aa","metadata":{},"outputs":[],"source":["# text_idf_bm25 = sc.broadcast(inverted_text.get_idf_bm25())\n","# title_idf_bm25 = sc.broadcast(inverted_title.get_idf_bm25())\n","\n","text_idf_bm25 = inverted_text.get_idf_bm25()\n","title_idf_bm25 = inverted_title.get_idf_bm25()"]},{"cell_type":"code","execution_count":null,"id":"a57deb38-fdc6-40a1-bfa2-e6cd2f8a7ed8","metadata":{},"outputs":[],"source":["# takes like a minute so run only when the normalized pageranks are needed:\n","# =========================================================================\n","text_pagerank = inverted_text.pagerank_normalized\n","title_pagerank = inverted_title.pagerank_normalized"]},{"cell_type":"code","execution_count":null,"id":"7a86b0cb-7e38-4da1-8f5f-d2ca68518c9b","metadata":{},"outputs":[],"source":["# text_avdl = sc.broadcast(inverted_text.get_avdl())\n","# title_avdl = sc.broadcast(inverted_title.get_avdl())\n","\n","text_avdl = inverted_text.get_avdl()\n","title_avdl = inverted_title.get_avdl()"]},{"cell_type":"code","execution_count":null,"id":"4db07db8","metadata":{},"outputs":[],"source":["# check if we're on the right pickle\n","print(len(text_doc_len))\n","print(len(title_idf_bm25))\n","print(title_avdl)\n","print(len(title_pagerank))"]},{"cell_type":"code","execution_count":null,"id":"36df9910-c1ed-41a3-afc1-4e39b9f3e956","metadata":{},"outputs":[],"source":["k1 = 0.1\n","k3 = 0\n","b = 0.75"]},{"cell_type":"code","execution_count":null,"id":"1740c012","metadata":{},"outputs":[],"source":["def retrieve_posting_list(query_word: str):\n","#     text_pl = inverted_text.read_a_posting_list(base_dir='.', w=query_word, bucket_name=bucket_name)\n","    title_pl = inverted_title.read_a_posting_list(base_dir='.', w=query_word, bucket_name=bucket_name)\n","    # return text_pl, title_pl\n","    return title_pl\n","\n","# def combine_title_and_text(token, pls):\n","#     text_pl, title_pl = pls\n","#     text_pl = sc.parallelize(text_pl)\n","#     text_pl_normalized = text_pl.map(lambda x: (x[0], ((x[1] / text_doc_len.get(x[0], float('inf'))))))\n","#     # text_pl_normalized = text_pl.map(lambda x: (x[0], ((x[1] / text_doc_len.get(x[0], float('inf'))) * text_idf[token])))\n","#     # text_pl_normalized = text_pl.map(lambda x: (x[0], ((x[1] / text_doc_len.get(x[0], float('inf'))) * text_idf[token] * text_pagerank.get(x[0], 0.001))))\n","#     title_pl = sc.parallelize(title_pl)\n","#     title_pl_normalized = title_pl.map(lambda x: (x[0], ((x[1] / title_doc_len.get(x[0], float('inf'))))))\n","#     # title_pl_normalized = title_pl.map(lambda x: (x[0], ((x[1] / title_doc_len.get(x[0], float('inf'))) * title_idf[token])))\n","#     # title_pl_normalized = title_pl.map(lambda x: (x[0], ((x[1] / title_doc_len.get(x[0], float('inf'))) * title_idf[token] * title_pagerank.get(x[0], 0.001))))\n","#     combined_rdd = text_pl_normalized.union(title_pl_normalized)\n","#     return combined_rdd\n","\n","def query(query: str):\n","    tokens = [token.group() for token in RE_WORD.finditer(query.lower())]\n","    tokens = [token for token in tokens if token not in all_stopwords]\n","    tokens = sc.parallelize(tokens)\n","    \n","    posting_lists = tokens.map(lambda x: (x, retrieve_posting_list(x))).flatMapValues(lambda x: x)\n","#     bm_25 on title:\n","    bm_25 = posting_lists.map(lambda x: (x[1][0], ((x[1][1] * (k1 + 1)) / (1 - b + (b * title_doc_len[x[1][0]] / title_avdl))) * title_idf_bm25[x[0]]))\n","    summed_rdd = bm_25.reduceByKey(lambda x, y: x + y)\n","    pr = summed_rdd.map(lambda x: (x[0], title_pagerank[x[0]]*x[1]))\n","    sorted_rdd = pr.sortBy(lambda pair: pair[1], ascending=False)\n","    sorted_rdd_title = sorted_rdd.zipWithIndex().filter(lambda x: x[1] < 1000).map(lambda x: x[0])\n","    \n","    print('hi')\n","    return (sorted_rdd_title.take(30))\n","    \n","# #     bm_25 on text:\n","#     bm_25 = sorted_rdd_title.map(lambda x: (x[1][0], ((x[1][1] * (k1 + 1)) / (1 - b + (b * text_doc_len[x[1][0]] / text_avdl))) * text_idf_bm25[x[0]]))\n","#     summed_rdd = bm_25.reduceByKey(lambda x, y: x + y)\n","#     pr = summed_rdd.map(lambda x: (x[0], text_pagerank[x[0]]*x[1]))\n","#     sorted_rdd = pr.sortBy(lambda pair: pair[1], ascending=False)\n","\n","    # posting_lists = [retrieve_posting_list(token) for token in tokens]\n","    # combined_posting_lists = [combine_title_and_text(*pl) for pl in posting_lists]\n","    # combined_rdd = sc.union(combined_posting_lists)\n","    # combined_rdd = sc.union(posting_lists)\n","    # summed_rdd = combined_rdd.reduceByKey(lambda x, y: x + y)\n","#     pr = summed_rdd.map(lambda x: (x[0], text_pagerank[x[0]]*x[1]))\n","#     sorted_rdd = pr.sortBy(lambda pair: pair[1], ascending=False)\n","\n","#     w/ out pr:\n","#     sorted_rdd = summed_rdd.sortBy(lambda pair: pair[1], ascending=False)\n","\n","    \n","#     return (sorted_rdd.take(30))"]},{"cell_type":"code","execution_count":null,"id":"f2290263","metadata":{},"outputs":[],"source":["query('orchard')"]},{"cell_type":"code","execution_count":null,"id":"0b6dd691","metadata":{"tags":[]},"outputs":[],"source":["# query('the Austro-Hungarian ultimatum to Serbia on 23 July')"]},{"cell_type":"code","execution_count":null,"id":"798ad77a-3cf9-4acd-9ee5-3f2fd2de5468","metadata":{},"outputs":[],"source":["# query('berlin wall')"]},{"cell_type":"code","execution_count":null,"id":"ef1c67f2-e556-4c6a-87fb-720f77d3951c","metadata":{},"outputs":[],"source":["# plplp = sc.parallelize([token for token in ['giọt', 'dunsby']]).map(lambda x: (x, retrieve_posting_list(x)))"]},{"cell_type":"code","execution_count":null,"id":"c25c8660-e067-4aff-9843-971e25a00efb","metadata":{"jupyter":{"outputs_hidden":true},"tags":[]},"outputs":[],"source":["# plplp.collect()"]},{"cell_type":"code","execution_count":null,"id":"2a81c181-2a00-4e5c-aa6d-a182ef33f2f2","metadata":{"tags":[]},"outputs":[],"source":["# plplp.flatMapValues(lambda x: x).map(lambda x: (x[1][0], ((x[1][1] / text_doc_len.get(x[1][0], float('inf'))) * text_idf[x[0]] * text_pagerank.get(x[1][0], 0.001)))).collect()"]},{"cell_type":"code","execution_count":null,"id":"dc89caa1-a8bf-4730-a07a-304e10942e38","metadata":{"jupyter":{"outputs_hidden":true},"tags":[]},"outputs":[],"source":["# inverted_text.df"]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"collapsed_sections":[],"name":"assignment3_gcp.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}